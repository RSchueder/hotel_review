{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "We have the objective to describe every hotel on Booking.com in 5 positve words and 5 negative words.\n",
    "\n",
    "Generally, for every review given, we want to extract the most important positive and negative descriptor words. Then we then rank these words based on frequency of appearance per hotel based on that hotel's reviews. In this way we can get a description of each hotel in 10 words. <br>\n",
    "\n",
    "The approach will be:\n",
    "* create a corpus per review that is a combination of positive and negative aspects of the review\n",
    "* create a tf-idf score for the top n words of each review\n",
    "* find the top 5 positive and top 5 negative words per hotel\n",
    "* make 2 additional models trained on only the positive and negative reviews respectively. The subsequent steps are the same as for 1 model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "nltk.download('wordnet') \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "dataset = pd.read_csv('./Hotel_Reviews.csv')\n",
    "\n",
    "hotel_id = None\n",
    "if hotel_id is not None:\n",
    "    print(dataset['Hotel_Name'].unique()[hotel_id])\n",
    "    dataset = dataset[dataset['Hotel_Name'] == dataset['Hotel_Name'].unique()[hotel_id]]\n",
    "    print('number of reviews for this hotel = ' + str(len(dataset)))\n",
    "else:\n",
    "    print('training on all hotels!')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of extra stop words based on experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "new_words = ['go', 'nothing']\n",
    "stop_words = stop_words.union(new_words, stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean the dataset \n",
    "to clean we:\n",
    "* create a corpus containing a combination of positive and negative reviews\n",
    "* get rid of all instances of absent positive or negative reviews, and replace the 'No Review' blank with ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"review\"] = dataset['Positive_Review'] + dataset['Negative_Review']\n",
    "dataset[\"review\"] = dataset[\"review\"].apply(lambda x: str(x).replace(\"No Negative\", \"\").replace(\"No Positive\", \"\"))\n",
    "\n",
    "dataset['Positive_Review'] = dataset['Positive_Review'].apply(lambda x: str(x).replace(\"No Negative\", '').replace(\"No Positive\", ''))\n",
    "dataset['Negative_Review'] = dataset['Negative_Review'].apply(lambda x: str(x).replace(\"No Negative\", '').replace(\"No Positive\", ''))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise removal and normalization\n",
    "\n",
    "We remove noise from the review corpus and we normalize in order to remove sparsity in the word matrix. \n",
    "\n",
    "Normalization removes multiple occurrances or representations of a word. The stemming technique normalizes by removing suffixes, and the lemmatization echnique examines the roots of words for this reduction.\n",
    "\n",
    "We also want to remove stopwords, which are filler words that occur very often and contain no value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(var):\n",
    "    text = re.sub('[^a-zA-Z]', ' ', var)\n",
    "    \n",
    "    #Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    #remove tags\n",
    "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    ##Convert to list from string\n",
    "    text = text.split()\n",
    "    \n",
    "    ##Stemming\n",
    "    #s=PorterStemmer()\n",
    "    \n",
    "    #Lemmatisation\n",
    "    lem = WordNetLemmatizer()\n",
    "    \n",
    "    text = [lem.lemmatize(word) for word in text if not word in  \n",
    "            stop_words]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Positive_Review_norm'] = dataset['Positive_Review'].apply(clean_corpus)\n",
    "dataset['Negative_Review_norm'] = dataset['Negative_Review'].apply(clean_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the difference between original and normalized \n",
    "id = 2\n",
    "print(dataset['Positive_Review'].iloc[id])\n",
    "print('\\n')\n",
    "print(dataset['Positive_Review_norm'].iloc[id])\n",
    "print('\\n')\n",
    "print(dataset['Negative_Review'].iloc[id])\n",
    "print('\\n')\n",
    "print(dataset['Negative_Review_norm'].iloc[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a series of strings, each string containing a review. Collectively they are a corpus\n",
    "corpus_pos = dataset['Positive_Review_norm']\n",
    "corpus_neg = dataset['Negative_Review_norm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a corpus of words that has been normalized and has had noise removed. There is a positive and a negative corpus per review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preparation - Tokenization and vectorization\n",
    "Now the corpus has to be converted into a normalized format that the algorithms can interpret without tricky aspect of language causing problems.\n",
    "\n",
    "Now we need to create a vector of word counts (a bag of words). This turns a series of strings into a vector of (unique?) words. <br>\n",
    "\n",
    "description of variables used to define vectorizer:\n",
    "\n",
    "max_df — When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). This is to ensure that we only have words relevant to the context and not commonly used words.\n",
    "\n",
    "max_features — determines the number of columns in the matrix.\n",
    "\n",
    "n-gram range — we would want to look at a list of single words, two words (bi-grams) and three words (tri-gram) combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count vectorizer object\n",
    "cv_pos = CountVectorizer(max_df = 0.8, stop_words = stop_words, max_features = 10000, ngram_range = (1,3))\n",
    "cv_neg = CountVectorizer(max_df = 0.8, stop_words = stop_words, max_features = 10000, ngram_range = (1,3))\n",
    "cv_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns a sparse matrix representation of the counts\n",
    "# to fit a count vectorizor object to a corpus returns a matrix of n_documents, n_words in vector\n",
    "# cv = count vectorizer object\n",
    "# X_pos = sparse vocabulary matrix\n",
    "\n",
    "X_pos = cv_pos.fit_transform(corpus_pos)\n",
    "X_neg = cv_neg.fit_transform(corpus_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sparse vocabulary matrix, or a word count vector, is returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cv_pos.vocabulary_.keys())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-documents x n_words, with counts, as a sparse matrix\n",
    "X_pos\n",
    "len(cv_pos.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine top uni and bigrams of encoded vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most frequently occuring words\n",
    "def get_top_n_words(corpus, n=None):\n",
    "    # fit the count vectorizer to the corpus so it can be used to do things\n",
    "    cv = CountVectorizer().fit(corpus)\n",
    "    # encoded vector\n",
    "    bag_of_words = cv.transform(corpus)\n",
    "    # number of words\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in      \n",
    "                   cv.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                       reverse=True)\n",
    "    return words_freq[:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert most freq words to dataframe for plotting bar plot\n",
    "top_words = get_top_n_words(corpus_pos, n=20)\n",
    "top_df = pandas.DataFrame(top_words)\n",
    "top_df.columns=[\"Word\", \"Freq\"]\n",
    "\n",
    "#Barplot of most freq words\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "g = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df)\n",
    "g.set_xticklabels(g.get_xticklabels(), rotation=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Most frequently occuring Tri-grams\n",
    "def get_top_n3_words(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(3,3),  \n",
    "            max_features=2000).fit(corpus)\n",
    "    bag_of_words = vec1.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_words = get_top_n3_words(corpus_pos, n=20)\n",
    "top3_df = pandas.DataFrame(top3_words)\n",
    "top3_df.columns=[\"Tri-gram\", \"Freq\"]\n",
    "print(top3_df)\n",
    "\n",
    "#Barplot of most freq Bi-grams\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "h=sns.barplot(x=\"Tri-gram\", y=\"Freq\", data=top3_df)\n",
    "h.set_xticklabels(h.get_xticklabels(), rotation=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top3_words = get_top_n3_words(corpus_neg, n=20)\n",
    "top3_df = pandas.DataFrame(top3_words)\n",
    "top3_df.columns=[\"Tri-gram\", \"Freq\"]\n",
    "print(top3_df)\n",
    "\n",
    "#Barplot of most freq Bi-grams\n",
    "import seaborn as sns\n",
    "sns.set(rc={'figure.figsize':(13,8)})\n",
    "h=sns.barplot(x=\"Tri-gram\", y=\"Freq\", data=top3_df)\n",
    "h.set_xticklabels(h.get_xticklabels(), rotation=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing TD-IDF\n",
    "\n",
    "Our vectorized corpus contains a large number of words, now we need to quantify their impact and frequency. Not all words are important, so we need to filter out the unimportant ones using an algorithm more sophisticated than 'frequency'. For this we use TD-IDF.\n",
    "\n",
    "TF = Frequency of a term in a document / total number of terms in the document\n",
    "\n",
    "IDF = log(total documents) / # of documents with the term\n",
    "\n",
    "We are fitting the IDF (tfidf) object based on some sample documents we expect to be representative of the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TfidfTransformer object\n",
    "tfidf_transformer_pos = TfidfTransformer(smooth_idf=True, use_idf = True)\n",
    "# fit an IDF to a sparse matrix  \n",
    "# X_pos was the return of the countvectorizor object fit on a corpus ( a sparse matrix)\n",
    "tfidf_transformer_pos.fit(X_pos)\n",
    "\n",
    "tfidf_transformer_neg = TfidfTransformer(smooth_idf=True, use_idf = True)\n",
    "tfidf_transformer_neg.fit(X_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TF-IDF scores will allow us to extract the words with the highest scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the TD-IDF in descending order\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key = lambda x: (x[1], x[0]), reverse = True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn = 10):\n",
    "    '''\n",
    "    get the feature names of top items based on tf-idf score\n",
    "    '''\n",
    "    sorted_items = sorted_items[:topn]\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    for idx, score in sorted_items:\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "    results = {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]] = score_vals[idx]\n",
    "    return results\n",
    "\n",
    "def get_word_freq(corpus, n=None):\n",
    "    vec1 = CountVectorizer(ngram_range=(3,3),  \n",
    "            max_features=2000).fit(corpus)\n",
    "    bag_of_words = vec1.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n",
    "                  vec1.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], \n",
    "                reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first try with single example invoking tfidf_transformer_pos.transform() to see what it does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 20 # document id\n",
    "doc = corpus_pos.iloc[id]\n",
    "\n",
    "# cv_pos has already been trained\n",
    "# generate a vector of idf scores for all words in the vocabulary\n",
    "feature_names = cv_pos.get_feature_names()\n",
    "tf_idf_vector=tfidf_transformer_pos.transform(cv_pos.transform([doc]))\n",
    "sorted_items = sort_coo(tf_idf_vector.tocoo())\n",
    "keywords = extract_topn_from_vector(feature_names, sorted_items, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Positive_Review'].iloc[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 5\n",
    "doc = corpus_neg.iloc[id]\n",
    "\n",
    "feature_names = cv_neg.get_feature_names()\n",
    "tf_idf_vector=tfidf_transformer_neg.transform(cv_neg.transform([doc]))\n",
    "sorted_items = sort_coo(tf_idf_vector.tocoo())\n",
    "keywords = extract_topn_from_vector(feature_names, sorted_items, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and apply the single model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have laid out the components of the approach and examined the data, we can train an apply the model to the reviews. After this we can aggregate common words per hotel. <br>\n",
    "Here we will do the training on text that is a combination of positive and negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = dataset['review'].apply(clean_corpus)\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus)\n",
    "tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf = True)\n",
    "tfidf_transformer.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement the classifier across all documents.<br>\n",
    "**This is the most time-consuming step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_n_tfidf(doc, cv, tfidf_transformer, n):\n",
    "    '''\n",
    "    returns the top n keywords for a document\n",
    "    '''\n",
    "    feature_names = cv.get_feature_names()\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "    sorted_items = sort_coo(tf_idf_vector.tocoo())\n",
    "    keywords = extract_topn_from_vector(feature_names, sorted_items, n)\n",
    "    return ', '.join(list(keywords.keys())), ', '.join([str(keywords[word]) for word in keywords.keys()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = []\n",
    "negative = []\n",
    "for ind in range(len(dataset.index)):\n",
    "    positive.append(top_n_tfidf(dataset['Positive_Review_norm'].iloc[ind], cv, tfidf_transformer, 5))\n",
    "    negative.append(top_n_tfidf(dataset['Negative_Review_norm'].iloc[ind], cv, tfidf_transformer, 5))\n",
    "\n",
    "dataset['Positive_summary'] = positive\n",
    "dataset['Negative_summary'] = negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a summary of each review and we can save it as an appendix to the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('per_review_summary_1model.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final summary classification with single classifier\n",
    "We now should run the classifier on subsets of text representing the summaries of each review to extract summaries per hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dataset = pd.DataFrame([])\n",
    "positive = []\n",
    "negative = []\n",
    "hotels = []\n",
    "for hid in dataset['Hotel_Name'].unique():\n",
    "    dataset_hid = dataset[dataset['Hotel_Name'] == hid]\n",
    "    \n",
    "    # create a corpus containing the keywords of all reviews for this hotel\n",
    "    pos_corpus = [ii for ii in dataset_hid['Positive_summary']]\n",
    "    neg_corpus = [ii for ii in dataset_hid['Negative_summary']]\n",
    "    pos_corpus = ' '.join(pos_corpus).replace(',','')\n",
    "    neg_corpus = ' '.join(neg_corpus).replace(',','')\n",
    "    # the TF-IDF tends to give less emphasis to those words appearing a lot of times in the corpus\n",
    "    # we actually want this to be more of a frequency analysis than an importance analysis, since \n",
    "    # alll words are equally important\n",
    "    positive.append(get_word_freq(pos_corpus, 5))\n",
    "    negative.append(get_word_freq(neg_corpus, 5))\n",
    "\n",
    "    #positive.append(top_n_tfidf(pos_corpus, cv, tfidf_transformer, 5))\n",
    "    #negative.append(top_n_tfidf(neg_corpus, cv, tfidf_transformer, 5))\n",
    "    \n",
    "    hotels.append(hid)\n",
    "\n",
    "summary_dataset['Hotel_name'] = hotels\n",
    "summary_dataset['Positive_summary'] = positive\n",
    "summary_dataset['Negative_summary'] = negative\n",
    "\n",
    "dataset.to_csv('./review_summary.csv')\n",
    "summary_dataset.to_csv('./hotel_review_summary_1model.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final summary classification with multiple classifier\n",
    "* It could matter whether the model is trained on only positive, only negative, or reviews only from certain prices, or reviews from single or multiple hotels. We will now try to make a positive review model and a negative reviw model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_pos = dataset['Positive_Review'].apply(clean_corpus)\n",
    "corpus_neg = dataset['Negative_Review'].apply(clean_corpus)\n",
    "\n",
    "cv_pos = CountVectorizer()\n",
    "X_pos = cv_pos.fit_transform(corpus_pos)\n",
    "\n",
    "cv_neg = CountVectorizer()\n",
    "X_neg = cv_neg.fit_transform(corpus_neg)\n",
    "\n",
    "tfidf_transformer_pos = TfidfTransformer(smooth_idf=True, use_idf = True)\n",
    "tfidf_transformer_pos.fit(X_pos)\n",
    "\n",
    "tfidf_transformer_neg = TfidfTransformer(smooth_idf=True, use_idf = True)\n",
    "tfidf_transformer_neg.fit(X_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = []\n",
    "negative = []\n",
    "for ind in range(len(dataset.index)):\n",
    "    positive.append(top_n_tfidf(corpus_pos[ind], cv_pos, tfidf_transformer_pos, 5))\n",
    "    negative.append(top_n_tfidf(corpus_neg[ind], cv_neg, tfidf_transformer_neg, 5))\n",
    "\n",
    "dataset['Positive_summary'] = positive\n",
    "dataset['Negative_summary'] = negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dataset = pd.DataFrame([])\n",
    "positive = []\n",
    "negative = []\n",
    "hotels = []\n",
    "for hid in dataset['Hotel_Name'].unique():\n",
    "    dataset_hid = dataset[dataset['Hotel_Name'] == hid]\n",
    "    pos_corpus = [ii for ii in dataset_hid['Positive_summary']]\n",
    "    neg_corpus = [ii for ii in dataset_hid['Negative_summary']]\n",
    "    \n",
    "    pos_corpus = ' '.join(pos_corpus).replace(',','')\n",
    "    neg_corpus = ' '.join(neg_corpus).replace(',','')\n",
    "    \n",
    "    positive.append(get_word_freq(pos_corpus, 5))\n",
    "    negative.append(get_word_freq(neg_corpus, 5))\n",
    "    \n",
    "    #positive.append(top_n_tfidf(pos_corpus, cv_pos, tfidf_transformer_pos, 5))\n",
    "    #negative.append(top_n_tfidf(neg_corpus, cv_neg, tfidf_transformer_neg, 5))\n",
    "    hotels.append(hid)\n",
    "\n",
    "summary_dataset['Hotel_name'] = hotels\n",
    "summary_dataset['Positive_summary'] = positive\n",
    "summary_dataset['Negative_summary'] = negative\n",
    "\n",
    "summary_dataset.to_csv('./hotel_review_summary_2model.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workflow is based on https://medium.com/analytics-vidhya/automated-keyword-extraction-from-articles-using-nlp-bfd864f41b34\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
